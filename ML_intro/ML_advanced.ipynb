{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving a realistic task with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to skip these theoretical reminders if you feel confident with the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: loss functions\n",
    "\n",
    "Training and quality assessment of the model is carried out on independent sets of examples. As a rule, existing examples are divided into two subsets: training (train) and test (test). The choice of split ratio is a compromise. Indeed, the large size of the training leads to better algorithms, but more noise when evaluating the model on the test. Conversely, a large test sample size leads to a less noisy quality assessment, but the trained models are less accurate.\n",
    "\n",
    "Many classification models predict a rating of membership to a positive class $ \\tilde {y} (x) \\in R $ (for example, the probability of being a class 1). After that, a decision is made on the class of the object by comparing the estimate with a certain threshold $ \\ theta $:\n",
    "\n",
    "$$y(x) = \n",
    "\\begin{cases}\n",
    "+1, &\\text{if} \\; \\tilde{y}(x) \\geq \\theta \\\\\n",
    "-1, &\\text{if} \\; \\tilde{y}(x) < \\theta\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In this case, we can consider metrics that can work with the initial response of the classifier. In the assignment, we will work with the AUC-ROC metric, which in this case can be considered as the proportion of incorrectly ordered pairs of objects sorted by ascending the predicted grade 1 grade. A detailed understanding of how the AUC-ROC metrics work for this exercise is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: selection of hyperparameters of the model\n",
    "\n",
    "In machine learning tasks, one should distinguish between model parameters and hyperparameters (structural parameters). Typically, model parameters are adjusted during training (for example, weights in a linear model or a decision tree structure), while hyper parameters are set in advance (for example, the value of the regularization force in a linear model or the maximum depth of a decision tree). Each model, as a rule, has a lot of hyperparameters and there are no universal sets of hyperparameters that work optimally in all tasks, so for each task you need to choose your own set.\n",
    "\n",
    "To optimize the hyperparameters, models often use __search on the grid (grid search)__: for each hyperparameter, several values are selected, then all combinations of values are selected and a combination is selected on which the model shows the best quality (in terms of the metric being optimized). However, in this case, it is necessary to correctly evaluate the constructed model, namely, to do the partition into a training and test sample. There are several schemes for how this can be implemented:\n",
    "\n",
    " - Break the available sample into training and test. In this case, the comparison of a large number of models in the enumeration of hyperparameters leads to a situation where the best model on the test subsample does not retain its qualities on the new data. We can say that there is a _transition_ on a test sample.\n",
    " - To eliminate the problem described above, you can split the data into 3 non-overlapping subsamples: training, validation, and test. Validation subsample is used to compare models, and test - for the final quality assessment and comparison of families of models with selected hyperparameters.\n",
    " - Another way to compare models is [cross-validation](http://bit.ly/1CHXsNH). There are various cross-validation schemes:\n",
    "  - Leave-One-Out\n",
    "  - K-Fold\n",
    "  - Repeated random sampling\n",
    "  \n",
    "Cross validation is computationally expensive, especially if you are iterating over a grid with a very large number of combinations. Given the finiteness of the time to perform the task, a number of compromises arise: \n",
    "  - the hyperparameter grid can be made more sparse by going through fewer values of each hyperparameter; however, you should not forget that in this case you can skip a good combination of hyperparameters;\n",
    "  - cross-validation can be done with a smaller number of splits or folds, but in this case the quality assessment becomes more noisy and the risk of choosing a non-optimal set of hyperparameters increases due to randomness of splitting\n",
    "  - hyperparameters can be optimized sequentially (greedily) - one by one, and not to go through all the combinations; such a strategy does not always lead to an optimal set;\n",
    "  - sort through not all combinations of hyperparameters, but a small number of randomly selected ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The task\n",
    "\n",
    "In this exercise, we will learn to train machine learning models, correctly set up experiments, select hyperparameters, compare and mix models.\n",
    "\n",
    "The goal to predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables (as opposed to remotely sensed data). The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form (not scaled) and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type.\n",
    "\n",
    "This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.\n",
    " \n",
    "The study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:\n",
    "\n",
    "1. Spruce/Fir\n",
    "2. Lodgepole Pine\n",
    "3. Ponderosa Pine\n",
    "4. Cottonwood/Willow\n",
    "5. Aspen\n",
    "6. Douglas-fir\n",
    "7. Krummholz\n",
    "\n",
    "The training set (11340 observations) contains both features and the Cover_Type.\n",
    "\n",
    "### Data Fields\n",
    "__Elevation__ - Elevation in meters  \n",
    "__Aspect__ - Aspect in degrees azimuth  \n",
    "__Slope__ - Slope in degrees  \n",
    "__Horizontal_Distance_To_Hydrology__ - Horz Dist to nearest surface water features  \n",
    "__Vertical_Distance_To_Hydrology__ - Vert Dist to nearest surface water features  \n",
    "__Horizontal_Distance_To_Roadways__ - Horz Dist to nearest roadway  \n",
    "__Hillshade_9am__ (0 to 255 index) - Hillshade index at 9am, summer solstice  \n",
    "__Hillshade_Noon__ (0 to 255 index) - Hillshade index at noon, summer solstice  \n",
    "__Hillshade_3pm__ (0 to 255 index) - Hillshade index at 3pm, summer solstice  \n",
    "__Horizontal_Distance_To_Fire_Points__ - Horz Dist to nearest wildfire ignition points  \n",
    "__Wilderness_Area__ - Wilderness area designation  \n",
    "__Soil_Type__ - Soil Type designation  \n",
    "__Cover_Type__ (7 types, integers 1 to 7) - Forest Cover Type designation  \n",
    "\n",
    "The wilderness areas are:\n",
    "\n",
    "1. Rawah Wilderness Area\n",
    "2. Neota Wilderness Area\n",
    "3. Comanche Peak Wilderness Area\n",
    "4. Cache la Poudre Wilderness Area\n",
    "\n",
    "The soil types are:\n",
    "\n",
    "1. Cathedral family - Rock outcrop complex, extremely stony.\n",
    "2. Vanet - Ratake families complex, very stony.\n",
    "3. Haploborolis - Rock outcrop complex, rubbly.\n",
    "4. Ratake family - Rock outcrop complex, rubbly.\n",
    "5. Vanet family - Rock outcrop complex complex, rubbly.\n",
    "6. Vanet - Wetmore families - Rock outcrop complex, stony.\n",
    "7. Gothic family.\n",
    "8. Supervisor - Limber families complex.\n",
    "9. Troutville family, very stony.\n",
    "10. Bullwark - Catamount families - Rock outcrop complex, rubbly.\n",
    "11. Bullwark - Catamount families - Rock land complex, rubbly.\n",
    "12. Legault family - Rock land complex, stony.\n",
    "13. Catamount family - Rock land - Bullwark family complex, rubbly.\n",
    "14. Pachic Argiborolis - Aquolis complex.\n",
    "15. unspecified in the USFS Soil and ELU Survey.\n",
    "16. Cryaquolis - Cryoborolis complex.\n",
    "17. Gateview family - Cryaquolis complex.\n",
    "18. Rogert family, very stony.\n",
    "19. Typic Cryaquolis - Borohemists complex.\n",
    "20. Typic Cryaquepts - Typic Cryaquolls complex.\n",
    "21. Typic Cryaquolls - Leighcan family, till substratum complex.\n",
    "22. Leighcan family, till substratum, extremely bouldery.\n",
    "23. Leighcan family, till substratum - Typic Cryaquolls complex.\n",
    "24. Leighcan family, extremely stony.\n",
    "25. Leighcan family, warm, extremely stony.\n",
    "26. Granile - Catamount families complex, very stony.\n",
    "27. Leighcan family, warm - Rock outcrop complex, extremely stony.\n",
    "28. Leighcan family - Rock outcrop complex, extremely stony.\n",
    "29. Como - Legault families complex, extremely stony.\n",
    "30. Como family - Rock land - Legault family complex, extremely stony.\n",
    "31. Leighcan - Catamount families complex, extremely stony.\n",
    "32. Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\n",
    "33. Leighcan - Catamount families - Rock outcrop complex, extremely stony.\n",
    "34. Cryorthents - Rock land complex, extremely stony.\n",
    "35. Cryumbrepts - Rock outcrop - Cryaquepts complex.\n",
    "36. Bross family - Rock land - Cryumbrepts complex, extremely stony.\n",
    "37. Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n",
    "38. Leighcan - Moran families - Cryaquolls complex, extremely stony.\n",
    "39. Moran family - Cryorthents - Leighcan family complex, extremely stony.\n",
    "40. Moran family - Cryorthents - Rock land complex, extremely stony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a very useful package for dealing with this sort of data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./forest_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** Usually, after loading a dataset, some preprocessing is always necessary. In this case, it will be as follows:\n",
    " - Save the target variable (the one we want to predict) into a separate variable $y$\n",
    " - Save the explanatory variables into a separate variable $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df.Cover_Type)\n",
    "\n",
    "X = # Select all the columns except the target here.\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - In this part we will work on binary classification problem. That is why we will select only the examples with Cover Type 1 and 2. Next we will classify examples with Cover Type 1 vs examples with Cover Type 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bin = X[(y == 1) | (y == 2)]\n",
    "y_bin = y[(y == 1) | (y == 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Note that not all features are numeric. In the beginning we will work only with numeric features. Save them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "# Select only the columns that are numerical here.\n",
    "]\n",
    "\n",
    "num_features = X_bin[numeric_cols] # Select only numeric features here\n",
    "num_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training classifiers on real features\n",
    "\n",
    "In this section, it will be necessary to work only with real attributes and the target variable.\n",
    "\n",
    "In the beginning we will look at how the selection of hyperparameters on the grid works and how the quality of the partitioning affects the quality. Now and further we will consider 4 algorithms:\n",
    " - [kNN](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    " - [DecisonTree](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    " - [SGD Linear Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n",
    " - [RandomForest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "To begin with, the first three algorithms will choose one hyperparameter, which we will optimize:\n",
    "  - kNN - number of neighbors (**n_neighbors**)\n",
    "  - DecisonTree - tree depth (**max_depth**)\n",
    "  - SGD Linear Classifier - optimized function (**loss**)\n",
    " \n",
    "Leave the default values for the remaining hyperparameters. For the selection of hyperparameters, use the search on the grid, which is implemented in the class [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). As a cross-validation scheme, use 5-Fold CV, which can be set using the class [KFoldCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold).\n",
    "\n",
    "![](https://i.stack.imgur.com/YWgro.gif)\n",
    "\n",
    "\n",
    "\n",
    "**Task 2:** For each algorithm, select the optimal values of the specified hyperparameters, to do so select the values that should be tested in the grid search.\n",
    "\n",
    "Then we build a graph of the average value of the quality of the cross-validation algorithm for a given value of the hyperparameter, which also display the confidence interval.\n",
    "\n",
    "To obtain the value of quality on each fold, the average value of quality and other useful information, we use the field [*cv results_*](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "\n",
    "Which algorithm has the highest average quality?\n",
    "\n",
    "Largest confidence interval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the values that should be checked during the grid search of optimal parameters\n",
    "classifiers = {\n",
    "    KNeighborsClassifier:   [{'n_neighbors': [<COMPLETE HERE>]}],\n",
    "    DecisionTreeClassifier: [{'max_depth'  : [<COMPLETE HERE>]}],\n",
    "    SGDClassifier:          [{'loss'       : [<COMPLETE HERE>]}]\n",
    "}\n",
    "\n",
    "trained_clfs = []\n",
    "\n",
    "for classifier, params in classifiers.items():\n",
    "    clf = GridSearchCV(classifier(), param_grid=params, cv=KFold(n_splits=5), return_train_score=True)\n",
    "    clf.fit(num_features.values, y_bin)\n",
    "    trained_clfs.append([clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=fig_size)\n",
    "mean_values = []\n",
    "stds = []\n",
    "names = []\n",
    "\n",
    "for clf in trained_clfs:\n",
    "    mean_vals = clf[0].cv_results_['mean_test_score']\n",
    "    std_vals = clf[0].cv_results_['std_test_score']\n",
    "    params = clf[0].cv_results_['params'][0]\n",
    "    \n",
    "    idx = np.argmin(clf[0].cv_results_['rank_test_score'])\n",
    "    opt_params = clf[0].cv_results_['params'][idx]\n",
    "    mean = clf[0].cv_results_['mean_test_score'][idx]\n",
    "    std = clf[0].cv_results_['std_test_score'][idx]\n",
    "    name = clf[0].estimator.__str__().split('(')[0] + ' , {}'.format(opt_params)\n",
    "    stds.append(std)\n",
    "    mean_values.append(mean)\n",
    "    names.append(name)\n",
    "\n",
    "plt.title('Performance of classifiers with the best optimized parameter value')\n",
    "plt.errorbar(np.arange(len(mean_values)), mean_values, stds, linestyle='None', marker='*')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('classifier')\n",
    "plt.xticks(np.arange(len(mean_values)), names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Now let's select the number of trees (**n_estimators**) in the RandomForest algorithm. As you might know, in general, Random Forest is less likely to overfit with an increase in the number of trees. Pick the number of trees from which the quality on cross-validation stabilizes. Note that to conduct this experiment, it is not necessary to train many random forests with different numbers of trees from scratch: train one random forest with the maximum interesting number of trees, and then consider subsets of trees of different sizes consisting of trees of the constructed forest (field [**estimators_**](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)). In further experiments, use the number of trees found.\n",
    "\n",
    "Fill in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "clfs = []\n",
    "\n",
    "for train_index, test_index in kf.split(num_features.values):\n",
    "    X_train, X_test = num_features.values[train_index], num_features.values[test_index]\n",
    "    y_train, y_test = y_bin[train_index], y_bin[test_index]\n",
    "    \n",
    "    clf = #COMPLETE HERE: Create random forest classifier with 300 trees\n",
    "    #COMPLETE HERE: Train random forest classifier here\n",
    "    \n",
    "    clfs.append([clf, X_test, y_test - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_by_fold = []\n",
    "\n",
    "for clf_n_data in clfs:\n",
    "    clf, x, y = clf_n_data\n",
    "    predictions = []\n",
    "    \n",
    "    for estimator in clf.estimators_:\n",
    "        predictions.append(<Make prediction here>)\n",
    "        \n",
    "    scores_by_fold.append([np.array(predictions), y])\n",
    "    \n",
    "quality_by_num_trees = []\n",
    "for num_trees in range(1, 301):\n",
    "    quality = []\n",
    "    \n",
    "    for predictions, labels in scores_by_fold:\n",
    "        vote = (predictions[:num_trees, :].mean(axis=0) > 0.5).astype(np.int64)\n",
    "        quality.append((vote == labels).sum() / float(len(labels)))\n",
    "        \n",
    "    quality_by_num_trees.append(quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "stds = []\n",
    "xs = []\n",
    "for i, num_trees in enumerate(range(1, 301)):\n",
    "    means.append(np.mean(quality_by_num_trees[i]))\n",
    "    stds.append(np.std(quality_by_num_trees[i]))\n",
    "    xs.append(num_trees)\n",
    "    \n",
    "plt.figure(figsize=fig_size)\n",
    "plt.plot(xs, means, 'b', label='accuracy')\n",
    "plt.title('Avg quality by num_trees')\n",
    "plt.xlabel('num_trees')\n",
    "plt.xlabel('accuracy')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.fill_between(xs, np.array(means) - np.array(stds), np.array(means) + np.array(stds), facecolor='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When learning algorithms, it is worth paying attention not only to their quality, but also how they deal with the data. In this problem, it turned out that some of the algorithms used are sensitive to the scale of features. To make sure that this could affect the quality, let's look at the meaning of the attributes themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** Look at the values of the features **Aspect**, **Slope**, **Hillshade_9am**. How is the data distributed? Which of the considered algorithms can be affected by this? Can scaling of the features affect these algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=fig_size)\n",
    "plt.hist(np.array(num_features.Elevation))\n",
    "plt.yscale('log')\n",
    "plt.ylabel('freqs')\n",
    "plt.xlabel('Elevation')\n",
    "plt.title('Elevation feature values distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the same graph for the Aspect, Slope and Hillshade_9am features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can scale attributes, for example, in one of the following ways:\n",
    " - $x_{new} = \\dfrac{x - \\mu}{\\sigma}$, where $\\mu, \\sigma$ — the mean and standard deviation of the characteristic value over the entire sample (see the function [scale](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html))\n",
    " - $x_{new} = \\dfrac{x - x_{min}}{x_{max} - x_{min}}$, where $[x_{min}, x_{max}]$ — minimum interval of characteristic values\n",
    "\n",
    "Similar scaling schemes are given in classes [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) and [MinMaxScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler).\n",
    " \n",
    "**Task 5:** Scale all numerical attributes using the standard scaler and select the optimal values of the hyperparameters in the same way as above.\n",
    "\n",
    "Has the quality of some algorithms changed and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = #COMPLETE HERE: Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute and plot average quality of the first three models with the normalised features \n",
    "#As well as the quality of the random forect classifier by num trees with the normalised features \n",
    "#You can reuse the previous code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6:** Now go through several hyperparameters across the grid and find the optimal combinations (best average quality value) for each algorithm in this case:\n",
    "  - KNN - number of neighbors (**n_neighbors**) and metric (**metric**)\n",
    "  - DecisonTree - tree depth (**max_depth**) and partitioning criterion (**criterion**)\n",
    "  - RandomForest - the partitioning criterion in trees (**criterion**) and the maximum number of considered features (**max_features**); use the trees found earlier\n",
    "  - SGDClassifier - optimized function (**loss**) and **penalty**\n",
    " \n",
    "Please note that this operation can be resource intensive and time consuming. How to optimize the selection of parameters on the grid is described in the section \"Selection of hyperparameters of the model\".\n",
    "\n",
    "Which algorithm has the best quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert values that should be checked during the grid search of optimal parameters\n",
    "\n",
    "classifiers = {\n",
    "    KNeighborsClassifier: [{'n_neighbors': [<COMPLETE HERE>],\n",
    "                            'metric': [<COMPLETE HERE>]\n",
    "                           }],\n",
    "    DecisionTreeClassifier: [{'max_depth': [<COMPLETE HERE>], \n",
    "                              'criterion': [<COMPLETE HERE>]\n",
    "                             }],\n",
    "    SGDClassifier: [{'loss': [<COMPLETE HERE>],\n",
    "                     'penalty': [<COMPLETE HERE>]}],\n",
    "    RandomForestClassifier: [{'criterion': [<COMPLETE HERE>],\n",
    "                              'max_features': [<COMPLETE HERE>],\n",
    "                              'n_estimators': [<COMPLETE HERE>]\n",
    "                             }]\n",
    "}\n",
    "\n",
    "trained_clfs = []\n",
    "\n",
    "for classifier, params in classifiers.items():\n",
    "    clf = GridSearchCV(classifier(), param_grid=params, cv=KFold(n_splits=5), return_train_score=True)\n",
    "    clf.fit(scaled_features, y_bin)\n",
    "    trained_clfs.append([clf, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=fig_size)\n",
    "mean_values = []\n",
    "stds = []\n",
    "names = []\n",
    "\n",
    "for clf in trained_clfs:\n",
    "    mean_vals = clf[0].cv_results_['mean_test_score']\n",
    "    std_vals = clf[0].cv_results_['std_test_score']\n",
    "    params = clf[0].cv_results_['params'][0]\n",
    "    \n",
    "    idx = np.argmin(clf[0].cv_results_['rank_test_score'])\n",
    "    opt_params = clf[0].cv_results_['params'][idx]\n",
    "    mean = clf[0].cv_results_['mean_test_score'][idx]\n",
    "    std = clf[0].cv_results_['std_test_score'][idx]\n",
    "    name = clf[0].estimator.__str__().split('(')[0]\n",
    "    stds.append(std)\n",
    "    mean_values.append(mean)\n",
    "    names.append(name)\n",
    "\n",
    "plt.title('Performance of classifiers with the best optimized parameter value')\n",
    "plt.errorbar(np.arange(len(mean_values)), mean_values, stds, linestyle='None', marker='*')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('classifier')\n",
    "plt.xticks(np.arange(len(mean_values)), names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7:** We will create [learning curves](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html) for the different algorithms, depicting the dependence of the quality on test and training samples on the number of objects on which models are trained. Look at the behavior of the curves and answer the questions:\n",
    "* Can the quality on the test sample decrease with an increase in the number of objects? And on the training? Why? \n",
    "* For what purposes can quality knowledge be used on the training part of the sample?\n",
    "* Which algorithm is better trained on fewer objects?\n",
    "* Can the addition of new objects significantly improve the quality of any of the algorithms or, with the existing data set for all algorithms, saturation occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from here: http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in trained_clfs:\n",
    "    name = clf[0].estimator.__str__().split('(')[0]\n",
    "    plot_learning_curve(clf[0].best_estimator_, 'Learning Curves ({})'.format(name), scaled_features, y_bin, cv=KFold(n_splits=5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding categorical features to models\n",
    "\n",
    "So far we have not used non-numeric attributes that are in dataset. Let's see if we did the right thing and whether the quality of the models will increase after adding these attributes.\n",
    "\n",
    "**Task 8:** Select all categorical features to be transformed using the one-hot-encoding method ([pandas.get_dummies](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) or [DictVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) / [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) from sklearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = [\n",
    "# Select only the columns that are categorical here.\n",
    "]\n",
    "\n",
    "cat_features = X_bin[cat_columns]\n",
    "cat_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(categories='auto')\n",
    "categorical_encoded_features = enc.fit_transform(cat_features).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features turned out to be quite numerous, in this work we will not re-select the optimal hyperparameters for models taking into account new signs (although it would be better to do it). \n",
    "\n",
    "We add the coded categorical to the scaled numerical features. \n",
    "\n",
    "**Task 9:** Train the algorithms with the best hyper-parameters found earlier. Did the addition of new features increase the quality? Measure quality as before using a 5-fold CV. For this it is convenient to use the function [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html#sklearn.cross_validation.cross_val_score).\n",
    "\n",
    "Is the best classifier now different than before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = np.hstack([X_bin, categorical_encoded_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifiers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixing models (blending)\n",
    "\n",
    "\n",
    "In all the preceding paragraphs, we obtained many strong models that can be quite different in nature (for example, the method of the nearest neighbors and the random forest). Often in practice it is possible to increase the quality of prediction by mixing different models. Let's see if this approach really gives an increase in quality.\n",
    "\n",
    "Choose from the constructed models of the two previous points two, which gave the highest quality on cross-validation (we denote them $ {clf}_1 $ and $ {clf}_2 $). Next, build a new classifier, whose answer on some object $ x $ will look like this:\n",
    "\n",
    "$$result(x) = {clf}_1(x) * \\alpha + {clf}_2(x) * (1 - \\alpha)$$\n",
    "\n",
    "where $ \\alpha $ is a hyper-parameter of the new classifier.\n",
    "\n",
    "**Task 10:**\n",
    "When implementing models, it is good practice to create sklearn-compatible classes. First, such an implementation will have a standard interface and will allow other people to train the models you have implemented easily. Secondly, it is possible to use any sklearn package functionality that accepts a model as input, for example, the class **GridSearchCV**, **learning_curve** and others.\n",
    "\n",
    "Create a classifier that is initialized with two arbitrary classifiers and the $ \\ alpha $ parameter. During training, such a classifier should train both basic models, and at the stage of prediction, knead predictions of basic models according to the formula indicated above.\n",
    "\n",
    "To create a custom classifier, you must inherit from the base classes:\n",
    "*[BaseEstimator](http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html), [ClassifierMixin](http://scikit-learn.org/stable/modules/generated/sklearn.base.ClassifierMixin.html)* and implement methods*\\_\\_init\\_\\_, fit, predict and predict_proba*. Example sklearn-compatible classifier with comments can be found [here](http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlendingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, clf1=None, clf2=None, alpha=None):\n",
    "        pass # IMPLEMENT ME\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        pass # IMPLEMENT ME\n",
    "    \n",
    "    def predict(self, x):\n",
    "        pass # IMPLEMENT ME\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        pass # IMPLEMENT ME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 11:** Select the value $ \\alpha $ for this classifier from the grid from 0 to 1. If the class is implemented correctly, then you can use *GridSearchCV*, as is the case with conventional classifiers.\n",
    "\n",
    "Did this approach increase in quality compared to models that were trained separately? Explain why even simple blending of models can influence the final quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(\n",
    "    BlendingClassifier(\n",
    "        clf1=trained_clfs[1][0],\n",
    "        clf2=trained_clfs[3][0]\n",
    "    ), param_grid=[{'alpha': np.linspace(0, 1, num=10)}],\n",
    "    cv=KFold(n_splits=5), return_train_score=True)\n",
    "\n",
    "trained = clf.fit(all_features, y_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=fig_size)\n",
    "plt.plot(trained.cv_results_['param_alpha'].data.astype(np.float32), trained.cv_results_['mean_test_score'], 'b', label='accuracy')\n",
    "plt.title('Mean folds accuracy')\n",
    "plt.xlabel('x')\n",
    "plt.xlabel('alpha')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.fill_between(trained.cv_results_['param_alpha'].data.astype(np.float32), trained.cv_results_['mean_test_score'] - trained.cv_results_['std_test_score'], trained.cv_results_['mean_test_score'] +  trained.cv_results_['std_test_score'], facecolor='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_clfs.append([trained])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models comparison\n",
    "\n",
    "After many models have been built, we have to compare them with each other. \n",
    "\n",
    "**Task 12:** Reflect back on what we did and draw conclusions about the classifiers in terms of how they dealt with the different features and the complexity of the model itself (what kind of hyperparameters the model has, whether changing the value of the hyperparameter greatly affects the quality of the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = []\n",
    "splits = []\n",
    "stds = []\n",
    "names = []\n",
    "\n",
    "for clf in trained_clfs:\n",
    "    mean_vals = clf[0].cv_results_['mean_test_score']\n",
    "    std_vals = clf[0].cv_results_['std_test_score']\n",
    "    params = clf[0].cv_results_['params'][0]\n",
    "    \n",
    "    idx = np.argmin(clf[0].cv_results_['rank_test_score'])\n",
    "    opt_params = clf[0].cv_results_['params'][idx]\n",
    "    mean = clf[0].cv_results_['mean_test_score'][idx]\n",
    "    std = clf[0].cv_results_['std_test_score'][idx]\n",
    "    name = clf[0].estimator.__str__().split('(')[0]\n",
    "    split = []\n",
    "    for i in range(5):\n",
    "        split.append(clf[0].cv_results_['split{}_test_score'.format(i)][idx])\n",
    "    stds.append(std)\n",
    "    mean_values.append(mean)\n",
    "    names.append(name)\n",
    "    splits.append(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=fig_size)\n",
    "results_df = pd.DataFrame(data=np.asarray(splits).T, columns=[names])\n",
    "ax = results_df.boxplot(figsize=(15, 8), return_type='axes', sym='k.')\n",
    "_ = plt.setp(ax.lines, linewidth=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus task \n",
    "Try to find the best classifier (using the procedure above) on the full dataset with all cover types. \n",
    "You are free to create derived features from existing ones, experiment with different methods (classes of sklearn) and even to apply deep methods here.\n",
    "You are not allowed to use additional data.\n",
    "You are allowed to use publically available code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
