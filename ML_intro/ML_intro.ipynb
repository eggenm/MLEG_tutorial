{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://newapplift-production.s3.amazonaws.com/comfy/cms/files/files/000/001/201/original/machine-learning-robots-dilbert.gif) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex1 Implementing Linear Regression\n",
    "\n",
    "In this first exercise we will be implementing a linear regression algorithm. For this exercise we have a set of ten points $(X_1,y_1) \\cdots (X_{10},y_{10})$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries and functions we will need for this exercise\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data\n",
    "np.random.seed(42)\n",
    "X = np.arange(1, 11).reshape(10, 1)\n",
    "y = 0.5*X**2+ 2.5*X+0.5+2*np.random.randn(10,1)\n",
    "ylim = [0,80]\n",
    "%matplotlib notebook\n",
    "fig_size = (8.,6.)\n",
    "plt.figure(figsize=fig_size)\n",
    "plt.scatter(X, y)\n",
    "plt.title('The Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder :** the goal in a regression task is to learn the mapping from the features (explanatory variables or input) to the response variable (or output). In our case we want to learn some function $f$ to make our predictions: \n",
    "\n",
    "$$y^{pred}:=f(X)$$ \n",
    "\n",
    "such that these predictions are close to the observed points $y$.\n",
    "\n",
    "In linear regression we assume that the mapping between the features and the response variables is linear thus we can define our function $f$ as follows: \n",
    "$$f(X):=w_1 X + w_0$$\n",
    "where $w_1$ and $w_0$ are two parameters.\n",
    "\n",
    "The goal is then to optimize these parameters so that our predictions fit the data that we have. The standard approach to do so is to define a loss function that determines the quality of our model and to use an optimization algorithm to find the parameters that minimize this loss function. The standard loss for a regression task is the mean squared error (MSE), so the problem we are trying to solve is the following:\n",
    "\n",
    "$$\\hat{w} = \\underset{w}{\\operatorname{argmin}} \\frac{1}{n}\\sum_{i=1}^{n}(y_i^{pred}-y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** To solve this optimization problem we will implement a gradient descent algorithm. Fill in the missing steps in the code.\n",
    "\n",
    "**Hint:** For the derivative just make sure you replace $y_i^{pred}$ by its definition in the loss function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing visualization of the results\n",
    "%matplotlib notebook\n",
    "fig_size = (8.,6.)\n",
    "fig = plt.figure(figsize=fig_size)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.set_ylim(ylim)\n",
    "plt.ion()\n",
    "plt.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "# Defining the model parameters.\n",
    "w1 = 0.\n",
    "w0 = 0.\n",
    "\n",
    "L = 0.01  # The learning Rate\n",
    "num_iter = 1000  # The number of iterations to perform gradient descent\n",
    "\n",
    "n = float(len(X)) # Number of elements in X\n",
    "\n",
    "# Main loop for the gradient descent algorithm\n",
    "for i in range(num_iter): \n",
    "    y_pred = # COMPLETE HERE: The current predicted value of Y\n",
    "    \n",
    "    D_w1 = # COMPLETE HERE: Derivative of the loss wrt w1\n",
    "    D_w0 = # COMPLETE HERE: Derivative of the loss wrt w0\n",
    "    \n",
    "    w1 = # COMPLETE HERE: Gradient descent update of w1\n",
    "    w0 = # COMPLETE HERE: Gradient descent update of w0\n",
    "    \n",
    "    # Visualizing the results\n",
    "    if (i+1) % 10 == 0:\n",
    "        ax.clear()\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('y')\n",
    "        ax.scatter(X, y) \n",
    "        ax.set_ylim(ylim)\n",
    "        if w0 <0:\n",
    "            lab = 'y={:4f}*x{:4f}'.format(w1,w0)\n",
    "        else:\n",
    "            lab = 'y={:4f}*x+{:4f}'.format(w1,w0)\n",
    "        ax.plot([np.min(X), np.max(X)], [np.min(y_pred), np.max(y_pred)], color='red',label=lab)  # regression line\n",
    "        ax.set_title('Linear regression, iteration {}'.format(i+1))\n",
    "        ax.legend(loc=2)\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the exercise we will [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)'s linear regression model, so you can carry on even if you have not finished the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn's linear regression model.\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "x = np.arange(1, 11, 0.1).reshape(-1, 1)\n",
    "pred = model.predict(x)\n",
    "plt.figure(figsize=fig_size)\n",
    "plt.plot(X, y, 'ro', x, pred)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim(ylim)\n",
    "axes.set_title(\"MSE: {:4f}\".format(mean_squared_error(y,model.predict(X))))\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to improve the accuracy of our model.\n",
    "\n",
    "The most basic way to do so is often just to add additional features (explanatory variables). Let's try for example to add the squares of the features. This means we are making the assumption that the mapping between the data $X$ and the  response variables $y$ is polynomial of degree $2$, thus we are now fitting a function $f_2$ defined as follows: \n",
    "$$f_2(X):=w_2 {X}^2+w_1 X + w_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Create a $10\\times2$ matrix X2 where the first column is the original $X$ data and the second column is $X^2$, then run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = # COMPLETE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "plt.figure(figsize=fig_size)\n",
    "plt.plot(X, y, 'ro',label='Data')\n",
    "plt.plot(x, pred,label='f, MSE: {:4f}'.format(mean_squared_error(y,model.predict(X))))\n",
    "model.fit(X2, y)\n",
    "x2 = np.arange(1, 11, 0.1)\n",
    "x2 = np.c_[x2, x2**2]\n",
    "pred2 = model.predict(x2)\n",
    "plt.plot(x2[:, 0], pred2,label='f2, MSE: {:4f}'.format(mean_squared_error(y,model.predict(X2))))\n",
    "plt.legend()\n",
    "plt.title(\"Comparison between f and f2\")\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if you take this reasoning even further. \n",
    "\n",
    "**Task 3:** In the following code try to increase $N$ the degree of the features that you are adding and find out how much you can improve the the fit of the model. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "XN = X\n",
    "xN = np.arange(1, 11, 0.1)\n",
    "for i in range(1,N):\n",
    "    XN = np.c_[XN,X**(i+1)]\n",
    "    xN = np.c_[xN,np.arange(1, 11, 0.1)**(i+1)]\n",
    "model.fit(XN, y)\n",
    "predN = model.predict(xN)\n",
    "plt.figure(figsize=fig_size)\n",
    "plt.plot(XN[:, 0], y, 'ro', xN[:, 0], predN)\n",
    "plt.title(\"Degree: {}, MSE: {:4f}\".format(N,mean_squared_error(y,model.predict(XN))))\n",
    "plt.ylim(ylim)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we chose $N=8$ as the MSE is pretty low. Suppose that we are sampling some new points from the same original distribution how well does our model fits to the new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "np.random.seed(50)\n",
    "X_val = X + 0.5\n",
    "XN_val = X_val\n",
    "for i in range(1,N):\n",
    "    XN_val = np.c_[XN_val,X_val**(i+1)]\n",
    "y_val =0.5*X_val**2+ 2.5*X_val+0.5+2*np.random.randn(10,1)\n",
    "plt.figure(figsize=fig_size)\n",
    "plt.plot(X, y, 'ro',label='original points')\n",
    "plt.plot(XN_val[:,0],y_val,'go',label='new points')\n",
    "plt.legend()\n",
    "plt.title(\"Degree: {}, Original MSE: {:4f}, New MSE: {:4f}\".format(N,mean_squared_error(y,model.predict(XN)),mean_squared_error(y_val,model.predict(XN_val))))\n",
    "plt.plot(xN[:, 0], predN)\n",
    "plt.ylim(ylim)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the most important messages of this exercise. If you make the model complicated enough it will be able to fit your data perfectly but you have to make sure that you are using a validation scheme for your model! Splitting the data into training, validation and test sets or cross-validation are both valid schemes to evaluate how well your model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we are not overfitting to our training data, to do so we will use our new set of points as the validation set. For the purpose of this exercise we will not be using a test set.\n",
    "\n",
    "**Reminder:** The training and validation sets are used during training, the test set is used only after you're done with training to evaluate your model on unseen data. The training set is used for fitting the parameters. Whereas the validation set is used to tune \"hyperparameters\" such as the complexity of the model we are choosing ($N$ in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of preveting overfitting in our model is to choose the model complexity appropriately for the type of data that we have.\n",
    "\n",
    "**Task 4:** Run the following code and decide on what you think is the optimal choice for $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = range(1,9)\n",
    "fig = plt.figure(figsize=fig_size)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.ion()\n",
    "plt.show()\n",
    "fig.canvas.draw()\n",
    "train_mses=[]\n",
    "val_mses=[]\n",
    "for N in Ns:\n",
    "    ax.clear()\n",
    "    X_val = X + 0.5\n",
    "    XN_val = X_val\n",
    "    XN = X\n",
    "    xN = np.arange(1, 11, 0.1).reshape(100,1)\n",
    "    for i in range(1,N):\n",
    "        XN = np.c_[XN,X**(i+1)]\n",
    "        xN = np.c_[xN,np.arange(1, 11, 0.1)**(i+1)]\n",
    "        XN_val = np.c_[XN_val,X_val**(i+1)]\n",
    "        \n",
    "    model.fit(XN, y)\n",
    "    predN = model.predict(xN)\n",
    "    ax.plot(X, y, 'ro',label='Training points')\n",
    "    ax.plot(XN_val[:,0],y_val,'go',label='Validation points')\n",
    "    plt.legend()\n",
    "    plt.title(\"Degree: {}, Training MSE: {:4f}, Validation MSE: {:4f}\".format(N,mean_squared_error(y,model.predict(XN)),mean_squared_error(y_val,model.predict(XN_val))))\n",
    "    train_mses.append(mean_squared_error(y,model.predict(XN)))\n",
    "    val_mses.append(mean_squared_error(y_val,model.predict(XN_val)))\n",
    "    ax.plot(xN[:, 0], predN)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.pause(0.5)\n",
    "    fig.canvas.draw()\n",
    "plt.figure(figsize=fig_size)\n",
    "plt.semilogy(Ns,train_mses,'o:',label='Training MSE')\n",
    "plt.semilogy(Ns,val_mses,'o:',label='Validation MSE')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other way of preveting overfitting in our model is to introduce a regularizer. This means introducing an additional term to the loss function that penalizes large weights. For the purpose of this example we will use ridge regression where the optimization problem is the following:\n",
    "\n",
    "$$\\hat{w} = \\underset{w}{\\operatorname{argmin}} \\frac{1}{n}\\sum_{i=1}^n(y_i^{pred}-y_i)^2+\\alpha\\sum_jw_j^2$$\n",
    "\n",
    "**Task 5:** Run the following code and decide on what you think is the optimal choice for $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas =np.logspace(1,6,11)\n",
    "fig = plt.figure(figsize=fig_size)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.ion()\n",
    "plt.show()\n",
    "fig.canvas.draw()\n",
    "train_mses=[]\n",
    "val_mses=[]\n",
    "for alpha in alphas:\n",
    "    model_ridge = Ridge(alpha)\n",
    "    ax.clear()\n",
    "    model_ridge.fit(XN, y)\n",
    "    predN = model_ridge.predict(xN)\n",
    "    ax.plot(X, y, 'ro',label='Training points')\n",
    "    ax.plot(XN_val[:,0],y_val,'go',label='Validation points')\n",
    "    plt.legend()\n",
    "    plt.title(\"Alpha: {},Degree: {}, Training MSE: {:4f}, Validation MSE: {:4f}\".format(alpha,N,mean_squared_error(y,model_ridge.predict(XN)),mean_squared_error(y_val,model_ridge.predict(XN_val))))\n",
    "    train_mses.append(mean_squared_error(y,model_ridge.predict(XN)))\n",
    "    val_mses.append(mean_squared_error(y_val,model_ridge.predict(XN_val)))\n",
    "    ax.plot(xN[:, 0], predN)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.pause(0.5)\n",
    "    fig.canvas.draw()\n",
    "plt.figure(figsize=fig_size)\n",
    "plt.loglog(alphas,train_mses,'o:',label='Training')\n",
    "plt.loglog(alphas,val_mses,'o:',label='Validation')\n",
    "plt.legend()\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex2 Implementing Logistic Regression\n",
    "In this first exercise we will be implementing a logistic regression algorithm. Be careful this is actually a **classification** algorithm.  \n",
    "\n",
    "We are solving a binary classification task, this means that every point belongs to one of two classes either $1$ or $0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Make sure you run this cell but there is no need to read the code\n",
    "####################################################################\n",
    "\n",
    "def gen_data(typ=\"gauss\",noise =0.4,seed=42):\n",
    "    noise=0.4\n",
    "    np.random.seed(seed)\n",
    "    n_samples = 100\n",
    "    data = np.ones((n_samples,3))\n",
    "    if typ == \"gauss\":\n",
    "        for i in range(n_samples):\n",
    "            if i < n_samples/2:\n",
    "                data[i,0] = np.random.randn() - 2\n",
    "                data[i,1] = np.random.randn() - 2\n",
    "                data[i,2] = 1\n",
    "            else:\n",
    "                data[i,0] = np.random.randn() + 2\n",
    "                data[i,1] = np.random.randn() + 2\n",
    "                data[i,2] = 0\n",
    "    elif typ == \"circle\":\n",
    "        radius = 5.\n",
    "        for i in range(n_samples):\n",
    "            if i < n_samples/2:\n",
    "                r = radius*0.5*np.random.rand()\n",
    "                angle = 2.*np.pi*np.random.rand()\n",
    "                data[i,0] = r*np.sin(angle)+(np.random.rand()*2.*radius-radius)*noise\n",
    "                data[i,1] = r*np.cos(angle)+(np.random.rand()*2.*radius-radius)*noise\n",
    "                data[i,2] = 1\n",
    "            else:\n",
    "                r = radius*0.7+radius*0.3*np.random.rand()\n",
    "                angle = 2.*np.pi*np.random.rand()\n",
    "                data[i,0] = r*np.sin(angle)\n",
    "                data[i,1] = r*np.cos(angle)\n",
    "                data[i,2] = 0\n",
    "    elif typ == \"xor\":\n",
    "        for i in range(n_samples):\n",
    "            data[i,0] = (np.random.rand()*10.-5)*noise\n",
    "            data[i,1] = (np.random.rand()*10.-5)*noise\n",
    "            data[i,2] = 1\n",
    "        data[np.where(data[:,0]*data[:,1]<0),2] =0 \n",
    "    elif typ == \"spiral\":\n",
    "        n = int(n_samples/2)\n",
    "        for i in range(n):\n",
    "            r = i/n *5\n",
    "            t = 1.75 *i/n*2*np.pi\n",
    "            data[i,0] = r*np.sin(t) + (np.random.rand()*2 - 1)*noise\n",
    "            data[i,1] = r*np.cos(t) + (np.random.rand()*2 - 1)*noise\n",
    "            data[i,2] = 1\n",
    "\n",
    "        for i in range(n):\n",
    "            r = i/n *5\n",
    "            t = 1.75 *i/n*2*np.pi + np.pi\n",
    "            data[n+i,0] = r*np.sin(t) + (np.random.rand()*2 - 1)*noise\n",
    "            data[n+i,1] = r*np.cos(t) + (np.random.rand()*2 - 1)*noise\n",
    "            data[n+i,2] = 0\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "    return data[:,0:2],data[:,2]\n",
    "\n",
    "def new_fig_lr(X,y,data,model,transform):\n",
    "    if transform is None:\n",
    "        def transform(X):\n",
    "            return X\n",
    "    preds = model.predict(data)\n",
    "        \n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label='Class 1')\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label='Class 0')\n",
    "    plt.legend()\n",
    "    x1_min, x1_max = X[:,0].min(), X[:,0].max(),\n",
    "    x2_min, x2_max = X[:,1].min(), X[:,1].max(),\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "    grid = transform(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "    probs = model.predict_prob(grid).reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, probs, [0,0.5,1.], alpha=0.2,colors=['C1','b']);\n",
    "    CS = plt.contour(xx1, xx2, probs, [0.5], linewidths=1, colors='black');\n",
    "    labels = ['Decision boundary']\n",
    "    for i in range(len(labels)):\n",
    "        CS.collections[i].set_label(labels[i])\n",
    "    plt.title(\"Training accuracy: {:4f}\".format((preds == y).mean()))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def new_fig_data_lr(X,y):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.scatter(X[np.where(y==1),0],X[np.where(y==1),1],label=\"Class 1\")\n",
    "    plt.scatter(X[np.where(y==0),0],X[np.where(y==0),1], label=\"Class 0\")\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple dataset. Just be careful in this case $X$ is a $N\\times 2$ matrix containing the coordinates of the points and $y$ is a $N$ dimensional vector with the corresponding labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = gen_data()\n",
    "new_fig_data_lr(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder :** Similarly to the linear regression case we want to learn some function $f$ to make our predictions: \n",
    "\n",
    "$$y^{pred}:=f(X)$$ \n",
    "\n",
    "such that these predictions are close to the observed labels $y$.\n",
    "\n",
    "Again very much like logistic regression we define a combination of the input features:\n",
    "$$z(X):=w_1 X_1 + w_2 X_2+ b$$\n",
    "\n",
    "What's different is that now the variable that we want to approach is binary, it either correponds to Class 1 or to Class 0.\n",
    "\n",
    "Logistic regression models the probability of a point belonging to the default class. So to obtain a probability from $h(x)$ we just feed it through a function that project $h(x)$ back to the range $0$ to $1$.\n",
    "\n",
    "This is the sigmoid function (or logistic function), this function is very important in ML so make sure you remember its definition:\n",
    "\n",
    "$$\\sigma(x):=\\frac{1}{1+\\exp(-x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sig = np.linspace(-10,10)\n",
    "y_sig = 1/(1+np.exp(-X_sig))\n",
    "plt.figure(figsize=fig_size)\n",
    "plt.plot(X_sig,y_sig)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"sig(X)\")\n",
    "plt.title('The sigmoid function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the definition of the predicted probability of the point belonging to the default class is: \n",
    "\n",
    "$$h(X_i):=\\sigma(X_i\\cdot w+ b)$$\n",
    "\n",
    "with $w$ a vector of parameters (weights) and $b$ the intercept. Based on this predicted probability our predicted labels $y^{pred}$ are simply $y^{pred}=1$ if $h(X)>0.5$ and $y^{pred}=0$ otherwise. \n",
    "\n",
    "The goal is then exactly like for the linear regression case, we want to optimize these parameters so that our predictions fit the data that we have. The standard approach to do so is to define a loss function that determines the quality of our model and to use an optimization algorithm to find the parameters that minimize this loss function. The standard loss for a binary classification task is the cross-entropy loss (log loss), so the problem we are trying to solve is the following:\n",
    "\n",
    "$$\\hat{w} = \\underset{w}{\\operatorname{argmin}} \\frac{1}{n}\\sum_{i=1}^{n}\\left[-y_i\\log(h(X_i))-(1-y_i)\\log(1-h(X_i))\\right]$$\n",
    "\n",
    "Try to look at what happens when $y_i=1$ and similary when $y_i=0$.\n",
    "\n",
    "This can be reformulated in vector form as follows: \n",
    "\n",
    "$$\\{\\hat{w},\\hat{b}\\} = \\underset{w,b}{\\operatorname{argmin}} \\frac{1}{n}-y^\\top\\log(h(X))-(1-y^\\top)\\log(1-h(X))$$\n",
    "\n",
    "whith $h(X)=\\sigma(X\\cdot w + b \\boldsymbol{1_n})$ where $\\boldsymbol{1_n}$ is a n-dimensional vector of ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** Fill in the missing code in the following definition of the Logisitc Regression class, then run the cell to check the results. The gradient of the loss wrt. $w$ is:\n",
    "\n",
    "$$\\nabla_w \\mathcal{L} = \\frac{1}{n}X^\\top\\cdot(h(X)-y)$$\n",
    "\n",
    "which is a 2 dimensional vector. The derivative of the loss wrt. $b$ is :\n",
    "\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial b}=\\frac{1}{n}\\boldsymbol{1_n}^\\top\\cdot(h(X)-y)=\\frac{1}{n}\\sum(h(X)-y)$$\n",
    "\n",
    "If you have time try to compute this gradient and derivative yourself, note that: $\\frac{d\\sigma(x)}{dx}=\\sigma(x)(1-\\sigma(x))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, L=0.01, num_iter=1000):\n",
    "        self.L = L\n",
    "        self.num_iter = num_iter\n",
    "           \n",
    "    def sigmoid(self, z):\n",
    "        sig = # COMPLETE HERE: with the definition of the sigmoid function\n",
    "        return sig\n",
    "    \n",
    "    def loss(self, h, y):\n",
    "        cur_loss = # COMPLETE HERE: with the definition of the cross-entropy loss \n",
    "        return cur_loss\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # Weights initialization\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        self.b = 0.\n",
    "        \n",
    "        # Main gradient descent loop\n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.w) + self.b\n",
    "            h = self.sigmoid(z)\n",
    "            gradient_w = # COMPLETE HERE: with the gradient of the loss wrt w\n",
    "            derivative_b = # COMPLETE HERE: with the derivative of the loss wrt b\n",
    "            self.w = # COMPLETE HERE: with the update step for w\n",
    "            self.b = # COMPLETE HERE: with the update step for b\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        return (self.sigmoid(np.dot(X, self.w)+self.b))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_prob(X).round()\n",
    "    \n",
    "# Running our LR model on our dataset\n",
    "model = LogisticRegression(L=0.01, num_iter=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "new_fig_lr(X,y,X,model,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at this new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = gen_data(\"xor\")\n",
    "new_fig_data_lr(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to fit our logistic regression model to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(L=0.01, num_iter=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "new_fig_lr(X,y,X,model,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can clearly see the two classes are not linearly seperable. \n",
    "\n",
    "**Task 2:** Try to come up with a way to transform the data so that it becomes linearly separable. Make sure that your new transformed data is a $N\\times C$ matrix, where $N$ is the number of datapoints and $C$ the number of features (ie:2 for the original data).\n",
    "\n",
    "**Hint:** Try squaring the coordinates, using trigonometric functions, combining the two coordinates into a new one by adding or multiplying them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(L=0.01, num_iter=1000)\n",
    "\n",
    "def transform(X):\n",
    "    transformed = # COMPLETE HERE: new transformed data\n",
    "    return transformed\n",
    "\n",
    "data = transform(X)\n",
    "model.fit(data, y)\n",
    "\n",
    "new_fig_lr(X,y,data,model,transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Task:** If you have time, you can play around with two other datasets. Just change the argument in the `gen_data` function to \"spiral\" or \"circular\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = gen_data(#CHOOSE DATASET)\n",
    "new_fig_data_lr(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(L=0.01, num_iter=1000)\n",
    "\n",
    "def transform(X):\n",
    "    transformed = # COMPLETE HERE: new transformed data\n",
    "    return transformed\n",
    "\n",
    "data = transform(X)\n",
    "model.fit(data, y)\n",
    "\n",
    "new_fig_lr(X,y,data,model,transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex3 Solving a realistic task with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to skip these theoretical reminders if you feel confident with the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: loss functions\n",
    "\n",
    "Training and quality assessment of the model is carried out on independent sets of examples. As a rule, existing examples are divided into two subsets: training (train) and test (test). The choice of split ratio is a compromise. Indeed, the large size of the training leads to better algorithms, but more noise when evaluating the model on the test. Conversely, a large test sample size leads to a less noisy quality assessment, but the trained models are less accurate.\n",
    "\n",
    "Many classification models predict a rating of membership to a positive class $ \\tilde {y} (x) \\in R $ (for example, the probability of being a class 1). After that, a decision is made on the class of the object by comparing the estimate with a certain threshold $ \\ theta $:\n",
    "\n",
    "$$y(x) = \n",
    "\\begin{cases}\n",
    "+1, &\\text{if} \\; \\tilde{y}(x) \\geq \\theta \\\\\n",
    "-1, &\\text{if} \\; \\tilde{y}(x) < \\theta\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In this case, we can consider metrics that can work with the initial response of the classifier. In the assignment, we will work with the AUC-ROC metric, which in this case can be considered as the proportion of incorrectly ordered pairs of objects sorted by ascending the predicted grade 1 grade. A detailed understanding of how the AUC-ROC metrics work for this exercise is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: selection of hyperparameters of the model\n",
    "\n",
    "In machine learning tasks, one should distinguish between model parameters and hyperparameters (structural parameters). Typically, model parameters are adjusted during training (for example, weights in a linear model or a decision tree structure), while hyper parameters are set in advance (for example, the value of the regularization force in a linear model or the maximum depth of a decision tree). Each model, as a rule, has a lot of hyperparameters and there are no universal sets of hyperparameters that work optimally in all tasks, so for each task you need to choose your own set.\n",
    "\n",
    "To optimize the hyperparameters, models often use __search on the grid (grid search)__: for each hyperparameter, several values are selected, then all combinations of values are selected and a combination is selected on which the model shows the best quality (in terms of the metric being optimized). However, in this case, it is necessary to correctly evaluate the constructed model, namely, to do the partition into a training and test sample. There are several schemes for how this can be implemented:\n",
    "\n",
    " - Break the available sample into training and test. In this case, the comparison of a large number of models in the enumeration of hyperparameters leads to a situation where the best model on the test subsample does not retain its qualities on the new data. We can say that there is a _transition_ on a test sample.\n",
    " - To eliminate the problem described above, you can split the data into 3 non-overlapping subsamples: training, validation, and test. Validation subsample is used to compare models, and test - for the final quality assessment and comparison of families of models with selected hyperparameters.\n",
    " - Another way to compare models is [cross-validation](http://bit.ly/1CHXsNH). There are various cross-validation schemes:\n",
    "  - Leave-One-Out\n",
    "  - K-Fold\n",
    "  - Repeated random sampling\n",
    "  \n",
    "Cross validation is computationally expensive, especially if you are iterating over a grid with a very large number of combinations. Given the finiteness of the time to perform the task, a number of compromises arise: \n",
    "  - the hyperparameter grid can be made more sparse by going through fewer values of each hyperparameter; however, you should not forget that in this case you can skip a good combination of hyperparameters;\n",
    "  - cross-validation can be done with a smaller number of splits or folds, but in this case the quality assessment becomes more noisy and the risk of choosing a non-optimal set of hyperparameters increases due to randomness of splitting\n",
    "  - hyperparameters can be optimized sequentially (greedily) - one by one, and not to go through all the combinations; such a strategy does not always lead to an optimal set;\n",
    "  - sort through not all combinations of hyperparameters, but a small number of randomly selected ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The task\n",
    "\n",
    "In this exercise, we will learn to train machine learning models, correctly set up experiments, select hyperparameters, compare and mix models.\n",
    "\n",
    "The goal to predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables (as opposed to remotely sensed data). The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form (not scaled) and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type.\n",
    "\n",
    "This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.\n",
    " \n",
    "The study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:\n",
    "\n",
    "1. Spruce/Fir\n",
    "2. Lodgepole Pine\n",
    "3. Ponderosa Pine\n",
    "4. Cottonwood/Willow\n",
    "5. Aspen\n",
    "6. Douglas-fir\n",
    "7. Krummholz\n",
    "\n",
    "The training set (11340 observations) contains both features and the Cover_Type.\n",
    "\n",
    "### Data Fields\n",
    "__Elevation__ - Elevation in meters  \n",
    "__Aspect__ - Aspect in degrees azimuth  \n",
    "__Slope__ - Slope in degrees  \n",
    "__Horizontal_Distance_To_Hydrology__ - Horz Dist to nearest surface water features  \n",
    "__Vertical_Distance_To_Hydrology__ - Vert Dist to nearest surface water features  \n",
    "__Horizontal_Distance_To_Roadways__ - Horz Dist to nearest roadway  \n",
    "__Hillshade_9am__ (0 to 255 index) - Hillshade index at 9am, summer solstice  \n",
    "__Hillshade_Noon__ (0 to 255 index) - Hillshade index at noon, summer solstice  \n",
    "__Hillshade_3pm__ (0 to 255 index) - Hillshade index at 3pm, summer solstice  \n",
    "__Horizontal_Distance_To_Fire_Points__ - Horz Dist to nearest wildfire ignition points  \n",
    "__Wilderness_Area__ - Wilderness area designation  \n",
    "__Soil_Type__ - Soil Type designation  \n",
    "__Cover_Type__ (7 types, integers 1 to 7) - Forest Cover Type designation  \n",
    "\n",
    "The wilderness areas are:\n",
    "\n",
    "1. Rawah Wilderness Area\n",
    "2. Neota Wilderness Area\n",
    "3. Comanche Peak Wilderness Area\n",
    "4. Cache la Poudre Wilderness Area\n",
    "\n",
    "The soil types are:\n",
    "\n",
    "1. Cathedral family - Rock outcrop complex, extremely stony.\n",
    "2. Vanet - Ratake families complex, very stony.\n",
    "3. Haploborolis - Rock outcrop complex, rubbly.\n",
    "4. Ratake family - Rock outcrop complex, rubbly.\n",
    "5. Vanet family - Rock outcrop complex complex, rubbly.\n",
    "6. Vanet - Wetmore families - Rock outcrop complex, stony.\n",
    "7. Gothic family.\n",
    "8. Supervisor - Limber families complex.\n",
    "9. Troutville family, very stony.\n",
    "10. Bullwark - Catamount families - Rock outcrop complex, rubbly.\n",
    "11. Bullwark - Catamount families - Rock land complex, rubbly.\n",
    "12. Legault family - Rock land complex, stony.\n",
    "13. Catamount family - Rock land - Bullwark family complex, rubbly.\n",
    "14. Pachic Argiborolis - Aquolis complex.\n",
    "15. unspecified in the USFS Soil and ELU Survey.\n",
    "16. Cryaquolis - Cryoborolis complex.\n",
    "17. Gateview family - Cryaquolis complex.\n",
    "18. Rogert family, very stony.\n",
    "19. Typic Cryaquolis - Borohemists complex.\n",
    "20. Typic Cryaquepts - Typic Cryaquolls complex.\n",
    "21. Typic Cryaquolls - Leighcan family, till substratum complex.\n",
    "22. Leighcan family, till substratum, extremely bouldery.\n",
    "23. Leighcan family, till substratum - Typic Cryaquolls complex.\n",
    "24. Leighcan family, extremely stony.\n",
    "25. Leighcan family, warm, extremely stony.\n",
    "26. Granile - Catamount families complex, very stony.\n",
    "27. Leighcan family, warm - Rock outcrop complex, extremely stony.\n",
    "28. Leighcan family - Rock outcrop complex, extremely stony.\n",
    "29. Como - Legault families complex, extremely stony.\n",
    "30. Como family - Rock land - Legault family complex, extremely stony.\n",
    "31. Leighcan - Catamount families complex, extremely stony.\n",
    "32. Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\n",
    "33. Leighcan - Catamount families - Rock outcrop complex, extremely stony.\n",
    "34. Cryorthents - Rock land complex, extremely stony.\n",
    "35. Cryumbrepts - Rock outcrop - Cryaquepts complex.\n",
    "36. Bross family - Rock land - Cryumbrepts complex, extremely stony.\n",
    "37. Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n",
    "38. Leighcan - Moran families - Cryaquolls complex, extremely stony.\n",
    "39. Moran family - Cryorthents - Leighcan family complex, extremely stony.\n",
    "40. Moran family - Cryorthents - Rock land complex, extremely stony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a very useful package for dealing with this sort of data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./forest_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** Usually, after loading a dataset, some preprocessing is always necessary. In this case, it will be as follows:\n",
    " - Save the target variable (the one we want to predict) into a separate variable $y$\n",
    " - Save the explanatory variables into a separate variable $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df.Cover_Type)\n",
    "\n",
    "X = # Select all the columns except the target here.\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - In this part we will work on binary classification problem. That is why we will select only the examples with Cover Type 1 and 2. Next we will classify examples with Cover Type 1 vs examples with Cover Type 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bin = X[(y == 1) | (y == 2)]\n",
    "y_bin = y[(y == 1) | (y == 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Note that not all features are numeric. In the beginning we will work only with numeric features. Save them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "# Select only the columns that are numerical here.\n",
    "]\n",
    "\n",
    "num_features = X_bin[numeric_cols] # Select only numeric features here\n",
    "num_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training classifiers on real features\n",
    "\n",
    "In this section, it will be necessary to work only with real attributes and the target variable.\n",
    "\n",
    "In the beginning we will look at how the selection of hyperparameters on the grid works and how the quality of the partitioning affects the quality. Now and further we will consider 4 algorithms:\n",
    " - [kNN](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    " - [DecisonTree](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    " - [SGD Linear Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n",
    " - [RandomForest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "To begin with, the first three algorithms will choose one hyperparameter, which we will optimize:\n",
    "  - kNN - number of neighbors (**n_neighbors**)\n",
    "  - DecisonTree - tree depth (**max_depth**)\n",
    "  - SGD Linear Classifier - optimized function (**loss**)\n",
    " \n",
    "Leave the default values for the remaining hyperparameters. For the selection of hyperparameters, use the search on the grid, which is implemented in the class [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). As a cross-validation scheme, use 5-Fold CV, which can be set using the class [KFoldCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold).\n",
    "\n",
    "![](https://i.stack.imgur.com/YWgro.gif)\n",
    "\n",
    "\n",
    "\n",
    "**Task 2:** For each algorithm, select the optimal values of the specified hyperparameters, to do so select the values that should be tested in the grid search.\n",
    "\n",
    "Then we build a graph of the average value of the quality of the cross-validation algorithm for a given value of the hyperparameter, which also display the confidence interval.\n",
    "\n",
    "To obtain the value of quality on each fold, the average value of quality and other useful information, we use the field [*cv results_*](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "\n",
    "Which algorithm has the highest average quality?\n",
    "\n",
    "Largest confidence interval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the values that should be checked during the grid search of optimal parameters\n",
    "classifiers = {\n",
    "    KNeighborsClassifier:   [{'n_neighbors': [<COMPLETE HERE>]}],\n",
    "    DecisionTreeClassifier: [{'max_depth'  : [<COMPLETE HERE>]}],\n",
    "    SGDClassifier:          [{'loss'       : [<COMPLETE HERE>]}]\n",
    "}\n",
    "\n",
    "trained_clfs = []\n",
    "\n",
    "for classifier, params in classifiers.items():\n",
    "    clf = GridSearchCV(classifier(), param_grid=params, cv=KFold(n_splits=5), return_train_score=True)\n",
    "    clf.fit(num_features.values, y_bin)\n",
    "    trained_clfs.append([clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=fig_size)\n",
    "mean_values = []\n",
    "stds = []\n",
    "names = []\n",
    "\n",
    "for clf in trained_clfs:\n",
    "    mean_vals = clf[0].cv_results_['mean_test_score']\n",
    "    std_vals = clf[0].cv_results_['std_test_score']\n",
    "    params = clf[0].cv_results_['params'][0]\n",
    "    \n",
    "    idx = np.argmin(clf[0].cv_results_['rank_test_score'])\n",
    "    opt_params = clf[0].cv_results_['params'][idx]\n",
    "    mean = clf[0].cv_results_['mean_test_score'][idx]\n",
    "    std = clf[0].cv_results_['std_test_score'][idx]\n",
    "    name = clf[0].estimator.__str__().split('(')[0] + ' , {}'.format(opt_params)\n",
    "    stds.append(std)\n",
    "    mean_values.append(mean)\n",
    "    names.append(name)\n",
    "\n",
    "plt.title('Performance of classifiers with the best optimized parameter value')\n",
    "plt.errorbar(np.arange(len(mean_values)), mean_values, stds, linestyle='None', marker='*')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('classifier')\n",
    "plt.xticks(np.arange(len(mean_values)), names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Now let's select the number of trees (**n_estimators**) in the RandomForest algorithm. As you might know, in general, Random Forest is less likely to overfit with an increase in the number of trees. Pick the number of trees from which the quality on cross-validation stabilizes. Note that to conduct this experiment, it is not necessary to train many random forests with different numbers of trees from scratch: train one random forest with the maximum interesting number of trees, and then consider subsets of trees of different sizes consisting of trees of the constructed forest (field [**estimators_**](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)). In further experiments, use the number of trees found.\n",
    "\n",
    "Fill in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "clfs = []\n",
    "\n",
    "for train_index, test_index in kf.split(num_features.values):\n",
    "    X_train, X_test = num_features.values[train_index], num_features.values[test_index]\n",
    "    y_train, y_test = y_bin[train_index], y_bin[test_index]\n",
    "    \n",
    "    clf = #COMPLETE HERE: Create random forest classifier with 300 trees\n",
    "    #COMPLETE HERE: Train random forest classifier here\n",
    "    \n",
    "    clfs.append([clf, X_test, y_test - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_by_fold = []\n",
    "\n",
    "for clf_n_data in clfs:\n",
    "    clf, x, y = clf_n_data\n",
    "    predictions = []\n",
    "    \n",
    "    for estimator in clf.estimators_:\n",
    "        predictions.append(<Make prediction here>)\n",
    "        \n",
    "    scores_by_fold.append([np.array(predictions), y])\n",
    "    \n",
    "quality_by_num_trees = []\n",
    "for num_trees in range(1, 301):\n",
    "    quality = []\n",
    "    \n",
    "    for predictions, labels in scores_by_fold:\n",
    "        vote = (predictions[:num_trees, :].mean(axis=0) > 0.5).astype(np.int64)\n",
    "        quality.append((vote == labels).sum() / float(len(labels)))\n",
    "        \n",
    "    quality_by_num_trees.append(quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "stds = []\n",
    "xs = []\n",
    "for i, num_trees in enumerate(range(1, 301)):\n",
    "    means.append(np.mean(quality_by_num_trees[i]))\n",
    "    stds.append(np.std(quality_by_num_trees[i]))\n",
    "    xs.append(num_trees)\n",
    "    \n",
    "plt.figure(figsize=fig_size)\n",
    "plt.plot(xs, means, 'b', label='accuracy')\n",
    "plt.title('Avg quality by num_trees')\n",
    "plt.xlabel('num_trees')\n",
    "plt.xlabel('accuracy')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.fill_between(xs, np.array(means) - np.array(stds), np.array(means) + np.array(stds), facecolor='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When learning algorithms, it is worth paying attention not only to their quality, but also how they deal with the data. In this problem, it turned out that some of the algorithms used are sensitive to the scale of features. To make sure that this could affect the quality, let's look at the meaning of the attributes themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** Look at the values of the features **Aspect**, **Slope**, **Hillshade_9am**. How is the data distributed? Which of the considered algorithms can be affected by this? Can scaling of the features affect these algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=fig_size)\n",
    "plt.hist(np.array(num_features.Elevation))\n",
    "plt.yscale('log')\n",
    "plt.ylabel('freqs')\n",
    "plt.xlabel('Elevation')\n",
    "plt.title('Elevation feature values distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the same graph for the Aspect, Slope and Hillshade_9am features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can scale attributes, for example, in one of the following ways:\n",
    " - $x_{new} = \\dfrac{x - \\mu}{\\sigma}$, where $\\mu, \\sigma$ — the mean and standard deviation of the characteristic value over the entire sample (see the function [scale](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html))\n",
    " - $x_{new} = \\dfrac{x - x_{min}}{x_{max} - x_{min}}$, where $[x_{min}, x_{max}]$ — minimum interval of characteristic values\n",
    "\n",
    "Similar scaling schemes are given in classes [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) and [MinMaxScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler).\n",
    " \n",
    "**Task 5:** Scale all numerical attributes using the standard scaler and select the optimal values of the hyperparameters in the same way as above.\n",
    "\n",
    "Has the quality of some algorithms changed and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = #COMPLETE HERE: Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute and plot average quality of the first three models with the normalised features \n",
    "#As well as the quality of the random forect classifier by num trees with the normalised features \n",
    "#You can reuse the previous code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6:** Now go through several hyperparameters across the grid and find the optimal combinations (best average quality value) for each algorithm in this case:\n",
    "  - KNN - number of neighbors (**n_neighbors**) and metric (**metric**)\n",
    "  - DecisonTree - tree depth (**max_depth**) and partitioning criterion (**criterion**)\n",
    "  - RandomForest - the partitioning criterion in trees (**criterion**) and the maximum number of considered features (**max_features**); use the trees found earlier\n",
    "  - SGDClassifier - optimized function (**loss**) and **penalty**\n",
    " \n",
    "Please note that this operation can be resource intensive and time consuming. How to optimize the selection of parameters on the grid is described in the section \"Selection of hyperparameters of the model\".\n",
    "\n",
    "Which algorithm has the best quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert values that should be checked during the grid search of optimal parameters\n",
    "\n",
    "classifiers = {\n",
    "    KNeighborsClassifier: [{'n_neighbors': [<COMPLETE HERE>],\n",
    "                            'metric': [<COMPLETE HERE>]\n",
    "                           }],\n",
    "    DecisionTreeClassifier: [{'max_depth': [<COMPLETE HERE>], \n",
    "                              'criterion': [<COMPLETE HERE>]\n",
    "                             }],\n",
    "    SGDClassifier: [{'loss': [<COMPLETE HERE>],\n",
    "                     'penalty': [<COMPLETE HERE>]}],\n",
    "    RandomForestClassifier: [{'criterion': [<COMPLETE HERE>],\n",
    "                              'max_features': [<COMPLETE HERE>],\n",
    "                              'n_estimators': [<COMPLETE HERE>]\n",
    "                             }]\n",
    "}\n",
    "\n",
    "trained_clfs = []\n",
    "\n",
    "for classifier, params in classifiers.items():\n",
    "    clf = GridSearchCV(classifier(), param_grid=params, cv=KFold(n_splits=5), return_train_score=True)\n",
    "    clf.fit(scaled_features, y_bin)\n",
    "    trained_clfs.append([clf, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=fig_size)\n",
    "mean_values = []\n",
    "stds = []\n",
    "names = []\n",
    "\n",
    "for clf in trained_clfs:\n",
    "    mean_vals = clf[0].cv_results_['mean_test_score']\n",
    "    std_vals = clf[0].cv_results_['std_test_score']\n",
    "    params = clf[0].cv_results_['params'][0]\n",
    "    \n",
    "    idx = np.argmin(clf[0].cv_results_['rank_test_score'])\n",
    "    opt_params = clf[0].cv_results_['params'][idx]\n",
    "    mean = clf[0].cv_results_['mean_test_score'][idx]\n",
    "    std = clf[0].cv_results_['std_test_score'][idx]\n",
    "    name = clf[0].estimator.__str__().split('(')[0]\n",
    "    stds.append(std)\n",
    "    mean_values.append(mean)\n",
    "    names.append(name)\n",
    "\n",
    "plt.title('Performance of classifiers with the best optimized parameter value')\n",
    "plt.errorbar(np.arange(len(mean_values)), mean_values, stds, linestyle='None', marker='*')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('classifier')\n",
    "plt.xticks(np.arange(len(mean_values)), names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7:** We will create [learning curves](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html) for the different algorithms, depicting the dependence of the quality on test and training samples on the number of objects on which models are trained. Look at the behavior of the curves and answer the questions:\n",
    "* Can the quality on the test sample decrease with an increase in the number of objects? And on the training? Why? \n",
    "* For what purposes can quality knowledge be used on the training part of the sample?\n",
    "* Which algorithm is better trained on fewer objects?\n",
    "* Can the addition of new objects significantly improve the quality of any of the algorithms or, with the existing data set for all algorithms, saturation occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from here: http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in trained_clfs:\n",
    "    name = clf[0].estimator.__str__().split('(')[0]\n",
    "    plot_learning_curve(clf[0].best_estimator_, 'Learning Curves ({})'.format(name), scaled_features, y_bin, cv=KFold(n_splits=5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding categorical features to models\n",
    "\n",
    "So far we have not used non-numeric attributes that are in dataset. Let's see if we did the right thing and whether the quality of the models will increase after adding these attributes.\n",
    "\n",
    "**Task 8:** Select all categorical features to be transformed using the one-hot-encoding method ([pandas.get_dummies](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) or [DictVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) / [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) from sklearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = [\n",
    "# Select only the columns that are categorical here.\n",
    "]\n",
    "\n",
    "cat_features = X_bin[cat_columns]\n",
    "cat_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(categories='auto')\n",
    "categorical_encoded_features = enc.fit_transform(cat_features).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features turned out to be quite numerous, in this work we will not re-select the optimal hyperparameters for models taking into account new signs (although it would be better to do it). \n",
    "\n",
    "We add the coded categorical to the scaled numerical features. \n",
    "\n",
    "**Task 9:** Train the algorithms with the best hyper-parameters found earlier. Did the addition of new features increase the quality? Measure quality as before using a 5-fold CV. For this it is convenient to use the function [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html#sklearn.cross_validation.cross_val_score).\n",
    "\n",
    "Is the best classifier now different than before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = np.hstack([X_bin, categorical_encoded_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifiers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixing models (blending)\n",
    "\n",
    "\n",
    "In all the preceding paragraphs, we obtained many strong models that can be quite different in nature (for example, the method of the nearest neighbors and the random forest). Often in practice it is possible to increase the quality of prediction by mixing different models. Let's see if this approach really gives an increase in quality.\n",
    "\n",
    "Choose from the constructed models of the two previous points two, which gave the highest quality on cross-validation (we denote them $ {clf}_1 $ and $ {clf}_2 $). Next, build a new classifier, whose answer on some object $ x $ will look like this:\n",
    "\n",
    "$$result(x) = {clf}_1(x) * \\alpha + {clf}_2(x) * (1 - \\alpha)$$\n",
    "\n",
    "where $ \\alpha $ is a hyper-parameter of the new classifier.\n",
    "\n",
    "**Task 10:**\n",
    "When implementing models, it is good practice to create sklearn-compatible classes. First, such an implementation will have a standard interface and will allow other people to train the models you have implemented easily. Secondly, it is possible to use any sklearn package functionality that accepts a model as input, for example, the class **GridSearchCV**, **learning_curve** and others.\n",
    "\n",
    "Create a classifier that is initialized with two arbitrary classifiers and the $ \\ alpha $ parameter. During training, such a classifier should train both basic models, and at the stage of prediction, knead predictions of basic models according to the formula indicated above.\n",
    "\n",
    "To create a custom classifier, you must inherit from the base classes:\n",
    "*[BaseEstimator](http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html), [ClassifierMixin](http://scikit-learn.org/stable/modules/generated/sklearn.base.ClassifierMixin.html)* and implement methods*\\_\\_init\\_\\_, fit, predict and predict_proba*. Example sklearn-compatible classifier with comments can be found [here](http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlendingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, clf1=None, clf2=None, alpha=None):\n",
    "        pass # IMPLEMENT ME\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        pass # IMPLEMENT ME\n",
    "    \n",
    "    def predict(self, x):\n",
    "        pass # IMPLEMENT ME\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        pass # IMPLEMENT ME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 11:** Select the value $ \\alpha $ for this classifier from the grid from 0 to 1. If the class is implemented correctly, then you can use *GridSearchCV*, as is the case with conventional classifiers.\n",
    "\n",
    "Did this approach increase in quality compared to models that were trained separately? Explain why even simple blending of models can influence the final quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(\n",
    "    BlendingClassifier(\n",
    "        clf1=trained_clfs[1][0],\n",
    "        clf2=trained_clfs[3][0]\n",
    "    ), param_grid=[{'alpha': np.linspace(0, 1, num=10)}],\n",
    "    cv=KFold(n_splits=5), return_train_score=True)\n",
    "\n",
    "trained = clf.fit(all_features, y_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=fig_size)\n",
    "plt.plot(trained.cv_results_['param_alpha'].data.astype(np.float32), trained.cv_results_['mean_test_score'], 'b', label='accuracy')\n",
    "plt.title('Mean folds accuracy')\n",
    "plt.xlabel('x')\n",
    "plt.xlabel('alpha')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.fill_between(trained.cv_results_['param_alpha'].data.astype(np.float32), trained.cv_results_['mean_test_score'] - trained.cv_results_['std_test_score'], trained.cv_results_['mean_test_score'] +  trained.cv_results_['std_test_score'], facecolor='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_clfs.append([trained])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models comparison\n",
    "\n",
    "After many models have been built, we have to compare them with each other. \n",
    "\n",
    "**Task 12:** Reflect back on what we did and draw conclusions about the classifiers in terms of how they dealt with the different features and the complexity of the model itself (what kind of hyperparameters the model has, whether changing the value of the hyperparameter greatly affects the quality of the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = []\n",
    "splits = []\n",
    "stds = []\n",
    "names = []\n",
    "\n",
    "for clf in trained_clfs:\n",
    "    mean_vals = clf[0].cv_results_['mean_test_score']\n",
    "    std_vals = clf[0].cv_results_['std_test_score']\n",
    "    params = clf[0].cv_results_['params'][0]\n",
    "    \n",
    "    idx = np.argmin(clf[0].cv_results_['rank_test_score'])\n",
    "    opt_params = clf[0].cv_results_['params'][idx]\n",
    "    mean = clf[0].cv_results_['mean_test_score'][idx]\n",
    "    std = clf[0].cv_results_['std_test_score'][idx]\n",
    "    name = clf[0].estimator.__str__().split('(')[0]\n",
    "    split = []\n",
    "    for i in range(5):\n",
    "        split.append(clf[0].cv_results_['split{}_test_score'.format(i)][idx])\n",
    "    stds.append(std)\n",
    "    mean_values.append(mean)\n",
    "    names.append(name)\n",
    "    splits.append(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=fig_size)\n",
    "results_df = pd.DataFrame(data=np.asarray(splits).T, columns=[names])\n",
    "ax = results_df.boxplot(figsize=(15, 8), return_type='axes', sym='k.')\n",
    "_ = plt.setp(ax.lines, linewidth=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus task \n",
    "Try to find the best classifier (using the procedure above) on the full dataset with all cover types. \n",
    "You are free to create derived features from existing ones, experiment with different methods (classes of sklearn) and even to apply deep methods here.\n",
    "You are not allowed to use additional data.\n",
    "You are allowed to use publically available code.\n",
    "\n",
    "You can send the results you have achieved in at [riccardo.delutio@geod.baug.ethz.ch], the best solution will receive a little present."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
